\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{devlin_bert_2019}
\citation{brown_language_2020}
\citation{jiang_mixtral_2024}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related works}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Use Case 1: Using Topic Cartography to summarize Prompt}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Framework: Topic Modeling Cartography}{3}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces BunkaTopics Architecture}}{4}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Method}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Use Case 2: Topic Modeling to clean datasets for Direct Preference Optimization (DPO) Optimization}{4}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Bunka Map of the Prompt-Collective mxbai-embed-large-v1}}{5}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Process of Direct Preference Optimisation (DPO) with and without Topic Filtering}}{5}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Method}{5}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison of results among the DPO-filtered model (Topic Neural Hermes), models fine-tuned on various subsets (NeuralHermes-2.5-Mistral-7B), and the base model (OpenHermes-2.5-Mistral-7B). Results can be found on the HuggingFace OpenLLM leaderboard. }}{6}{figure.caption.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Use Case 3: Analyzing bias using Semantic Framing Analysis}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Framework: Semantic Frames Cartography}{6}{subsection.4.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:a}{{5a}{7}{Filtering coefficient: 0.1}{figure.caption.6}{}}
\newlabel{sub@fig:a}{{a}{7}{Filtering coefficient: 0.1}{figure.caption.6}{}}
\newlabel{fig:b}{{5b}{7}{Filtering coefficient: 0.3}{figure.caption.6}{}}
\newlabel{sub@fig:b}{{b}{7}{Filtering coefficient: 0.3}{figure.caption.6}{}}
\newlabel{fig:c}{{5c}{7}{Filtering coefficient: 0.6}{figure.caption.6}{}}
\newlabel{sub@fig:c}{{c}{7}{Filtering coefficient: 0.6}{figure.caption.6}{}}
\newlabel{fig:d}{{5d}{7}{Filtering coefficient: 0.9}{figure.caption.6}{}}
\newlabel{sub@fig:d}{{d}{7}{Filtering coefficient: 0.9}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Bunka Frame Axis with different levels of filtering coefficient. Every point is a document of the dataset. Axis are defined by the difference between the embeddings of their two names. Distance is computed between the embeddings of the documents and the frames embeddings. There is no need to normalize given the fact that the axes are computed with the same method, but we centered around 0 for better readability. }}{7}{figure.caption.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Methods}{7}{subsection.4.2}\protected@file@percent }
\newlabel{fig:a}{{6a}{8}{Future or Past}{figure.caption.7}{}}
\newlabel{sub@fig:a}{{a}{8}{Future or Past}{figure.caption.7}{}}
\newlabel{fig:b}{{6b}{8}{Work or Leisure}{figure.caption.7}{}}
\newlabel{sub@fig:b}{{b}{8}{Work or Leisure}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Categorization performance between Bunka Frame Axis and zero-shot categorization using Mistral-7B-Instruct-v0.1 lines represent the different coefficient in order: 0.1, 0.2, 0.3, 0.4 with A. Future-Past and B. Work-Leisure}}{8}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Sample figure caption.}}{9}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion and Discussion}{9}{section.5}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{bib}
\bibcite{brown_language_2020}{{1}{}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, {McCandlish}, Radford, Sutskever, and Amodei}}}
\bibcite{devlin_bert_2019}{{2}{}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{jiang_mixtral_2024}{{3}{}{{Jiang et~al.}}{{Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, Lengyel, Bour, Lample, Lavaud, Saulnier, Lachaux, Stock, Subramanian, Yang, Antoniak, Scao, Gervet, Lavril, Wang, Lacroix, and Sayed}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Acknowledgment}{10}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Code and data Availability}{10}{section.7}\protected@file@percent }
\newlabel{fig:a}{{.8a}{11}{all-MiniLM-L6-v2}{figure.caption.11}{}}
\newlabel{sub@fig:a}{{a}{11}{all-MiniLM-L6-v2}{figure.caption.11}{}}
\newlabel{fig:b}{{.8b}{11}{bge-large-en-v1.5}{figure.caption.11}{}}
\newlabel{sub@fig:b}{{b}{11}{bge-large-en-v1.5}{figure.caption.11}{}}
\newlabel{fig:c}{{.8c}{11}{UAE-Large-V1}{figure.caption.11}{}}
\newlabel{sub@fig:c}{{c}{11}{UAE-Large-V1}{figure.caption.11}{}}
\newlabel{fig:d}{{.8d}{11}{multi-qa-mpnet-base-dot-v1}{figure.caption.11}{}}
\newlabel{sub@fig:d}{{d}{11}{multi-qa-mpnet-base-dot-v1}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {.8}{\ignorespaces  Topic Cartography with 10 clusters. We chose 3 specific nouns for visualization purposes. The Maps are created with different embedders A}}{11}{figure.caption.11}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Specific topics found for GPT4 answers}}{12}{table.caption.13}\protected@file@percent }
\newlabel{sample-table}{{1}{12}{Specific topics found for GPT4 answers}{table.caption.13}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}{\ignorespaces Python function to generate a prompt}}{12}{lstlisting.1}\protected@file@percent }
\newlabel{fig:a}{{.9a}{13}{politics/business \& past/future}{figure.caption.16}{}}
\newlabel{sub@fig:a}{{a}{13}{politics/business \& past/future}{figure.caption.16}{}}
\newlabel{fig:b}{{.9b}{13}{politics/business \& men/women}{figure.caption.16}{}}
\newlabel{sub@fig:b}{{b}{13}{politics/business \& men/women}{figure.caption.16}{}}
\newlabel{fig:c}{{.9c}{13}{men/women \& past/future}{figure.caption.16}{}}
\newlabel{sub@fig:c}{{c}{13}{men/women \& past/future}{figure.caption.16}{}}
\newlabel{fig:d}{{.9d}{13}{men/women \& work/leisure}{figure.caption.16}{}}
\newlabel{sub@fig:d}{{d}{13}{men/women \& work/leisure}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {.9}{\ignorespaces Frame Analysis on different axis. Our results suggest that the dataset is biased towards the concepts of future, business, men and work as compared to their opposite concepts. Also, the concept of men is more likely to be associated with the concept of the future than the concept of women. The concept of women is more likely to be associated with the concept of leisure than with the concept of work in comparison with the concept of men. }}{13}{figure.caption.16}\protected@file@percent }
\newlabel{fig:appendix1}{{.9}{13}{Frame Analysis on different axis. Our results suggest that the dataset is biased towards the concepts of future, business, men and work as compared to their opposite concepts. Also, the concept of men is more likely to be associated with the concept of the future than the concept of women. The concept of women is more likely to be associated with the concept of leisure than with the concept of work in comparison with the concept of men}{figure.caption.16}{}}
\newlabel{fig:a}{{.10a}{14}{Future or Past}{figure.caption.18}{}}
\newlabel{sub@fig:a}{{a}{14}{Future or Past}{figure.caption.18}{}}
\newlabel{fig:b}{{.10b}{14}{Work or Leisure}{figure.caption.18}{}}
\newlabel{sub@fig:b}{{b}{14}{Work or Leisure}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {.10}{\ignorespaces Categorization performance between Bunka Frame Axis and zero-shot categorization using Mistral-7B-Instruct-v0.1 based on the length of tokens. with A. Future-Past and B. Work-Leisure. We also note that the length of tokens has an impact on the classification results: there seems to be an optimum between 10 and 30 tokens where the Frame Axis model and the Mistral-7B-Instruct-v0.1 converge towards the same answer (Fig 8). We see a drop for the Future-Past categorization drops in the range [27-39] tokens. }}{14}{figure.caption.18}\protected@file@percent }
\gdef \@abspage@last{14}
